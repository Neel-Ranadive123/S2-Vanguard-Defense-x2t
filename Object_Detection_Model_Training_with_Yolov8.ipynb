{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c4b3580"
      },
      "source": [
        "# Object Detection Model Training with YOLOv8\n",
        "\n",
        "This notebook demonstrates the process of training a YOLOv8 object detection model on a custom dataset. The dataset is downloaded from Google Drive, unzipped, and then used for training. The training process involves sampling a subset of the training data for each epoch, evaluating the model periodically, and tracking metrics.\n",
        "\n",
        "## 1. Downloading and Extracting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8b5a221"
      },
      "source": [
        "# Download the dataset from Google Drive\n",
        "#!gdown --folder \"Link to the drive folder with the consolidated dataset\" --remaining-ok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "324bdb7d"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the name of the zip file downloaded from Google Drive\n",
        "zip_file_name = '/content/Consolidated Dataset/combined_dataset.zip'\n",
        "# Define the directory where the dataset will be extracted\n",
        "extract_dir = './extracted cmbined data'\n",
        "\n",
        "# Check if the zip file exists and extract its contents\n",
        "if os.path.exists(zip_file_name):\n",
        "    with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(f\"'{zip_file_name}' unzipped successfully to '{extract_dir}'.\")\n",
        "else:\n",
        "    print(f\"'{zip_file_name}' not found. Please ensure the download was successful.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db98baaf"
      },
      "source": [
        "## 2. Installing Dependencies\n",
        "\n",
        "Install the necessary libraries, including Ultralytics for YOLOv8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e4515e0"
      },
      "source": [
        "# Install the ultralytics library\n",
        "%pip install ultralytics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c80fef26"
      },
      "source": [
        "## 3. Training the YOLOv8 Model\n",
        "\n",
        "Train the YOLOv8 model on the extracted dataset. The training uses a sampling strategy to select a subset of images for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e617224a"
      },
      "source": [
        "# This notebook trains a YOLOv8 object detection model on a custom dataset.\n",
        "# The dataset is downloaded from a Google Drive folder, unzipped, and then used to train the model.\n",
        "# The training process involves sampling a subset of the training data for each epoch and evaluating the model periodically.\n",
        "# Metrics such as precision, recall, and mAP are tracked and saved to a CSV file.\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "import os\n",
        "import yaml\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Define the path to the data.yaml file within the extracted dataset\n",
        "data_yaml_path = './extracted cmbined data/data.yaml'\n",
        "\n",
        "# Update the paths in data.yaml to be relative to the data.yaml file's location\n",
        "# This is necessary for YOLOv8 to correctly locate the image and label directories\n",
        "try:\n",
        "    with open(data_yaml_path, 'r') as file:\n",
        "        data = yaml.safe_load(file)\n",
        "\n",
        "    data['train'] = '../train/images'\n",
        "    data['val'] = '../valid/images'\n",
        "    data['test'] = '../test/images'\n",
        "\n",
        "    with open(data_yaml_path, 'w') as file:\n",
        "        yaml.dump(data, file)\n",
        "    print(f\"Updated '{data_yaml_path}' with relative paths.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {data_yaml_path} not found. Please ensure the dataset is extracted correctly.\")\n",
        "    # Exit the cell execution if data.yaml is not found\n",
        "    raise\n",
        "\n",
        "# Instantiate a YOLO model with pretrained weights (YOLOv8n is a good starting point)\n",
        "# The model weights will be loaded from the last saved checkpoint in the training loop for resuming\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# Define the path for the training metrics CSV file where results will be saved\n",
        "metrics_csv_path = 'training_metrics.random_sample.csv'\n",
        "\n",
        "# Initialize a list to store the training metrics collected during evaluation\n",
        "metrics_data = []\n",
        "\n",
        "# Define the total number of training epochs and the interval for model evaluation\n",
        "total_epochs = 50\n",
        "eval_interval = 5\n",
        "# Define the number of images to sample from the training set for each epoch\n",
        "images_per_epoch = 2000\n",
        "\n",
        "# Get the list of all training image files from the extracted dataset\n",
        "train_images_dir = './extracted cmbined data/train/images'\n",
        "all_train_images = [os.path.join(train_images_dir, img) for img in os.listdir(train_images_dir) if img.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "# Create a temporary directory to store the sampled data for each epoch's training\n",
        "temp_data_dir = './temp_sampled_data'\n",
        "os.makedirs(temp_data_dir, exist_ok=True)\n",
        "\n",
        "# Create subdirectories for train and valid within the temporary directory to mimic the dataset structure\n",
        "temp_train_dir = os.path.join(temp_data_dir, 'train')\n",
        "temp_valid_dir = os.path.join(temp_data_dir, 'valid')\n",
        "os.makedirs(temp_train_dir, exist_ok=True)\n",
        "os.makedirs(temp_valid_dir, exist_ok=True)\n",
        "temp_train_images_dir = os.path.join(temp_train_dir, 'images')\n",
        "temp_train_labels_dir = os.path.join(temp_train_dir, 'labels')\n",
        "temp_valid_images_dir = os.path.join(temp_data_dir, 'valid', 'images') # Corrected path for valid images\n",
        "temp_valid_labels_dir = os.path.join(temp_data_dir, 'valid', 'labels') # Corrected path for valid labels\n",
        "os.makedirs(temp_train_images_dir, exist_ok=True)\n",
        "os.makedirs(temp_train_labels_dir, exist_ok=True)\n",
        "os.makedirs(temp_valid_images_dir, exist_ok=True) # Ensure valid image directory is created\n",
        "os.makedirs(temp_valid_labels_dir, exist_ok=True) # Ensure valid labels directory is created\n",
        "\n",
        "\n",
        "# Copy validation data to the temporary directory once before the training loop\n",
        "# This is done to avoid repeatedly copying the validation set in each epoch\n",
        "val_images_dir = './extracted cmbined data/valid/images'\n",
        "val_labels_dir = './extracted cmbined data/data/valid/labels' # Corrected path based on previous outputs\n",
        "\n",
        "for img_name in os.listdir(val_images_dir):\n",
        "    if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
        "        img_path = os.path.join(val_images_dir, img_name)\n",
        "        label_name = img_name.replace('.jpeg', '.txt').replace('.jpg', '.txt').replace('.png', '.txt')\n",
        "        label_path = os.path.join(val_labels_dir, label_name)\n",
        "\n",
        "        shutil.copy(img_path, temp_valid_images_dir)\n",
        "        if os.path.exists(label_path):\n",
        "            shutil.copy(label_path, temp_valid_labels_dir)\n",
        "\n",
        "# Create a temporary data.yaml file in the temporary directory for YOLOv8 to use\n",
        "temp_data_yaml_path = os.path.join(temp_data_dir, 'data.yaml')\n",
        "temp_data = data.copy()\n",
        "temp_data['train'] = 'train/images'  # Relative path within the temporary directory\n",
        "temp_data['val'] = 'valid/images'    # Relative path within the temporary directory\n",
        "with open(temp_data_yaml_path, 'w') as file:\n",
        "    yaml.dump(temp_data, file)\n",
        "\n",
        "# Access and modify the number of classes (nc) in the model based on the dataset\n",
        "# The dataset has 43 classes, but some labels go up to 49. We set to 50 to be safe.\n",
        "model.model.nc = data['nc'] if 'nc' in data and data['nc'] is not None else 43 # Default to 43 if nc is not specified or None\n",
        "# Assuming the last layer is the detect layer, update its nc and no\n",
        "if hasattr(model.model, 'model') and len(model.model.model) > 0 and hasattr(model.model.model[-1], 'nc'):\n",
        "    model.model.model[-1].nc = model.model.nc\n",
        "    model.model.model[-1].no = model.model.nc + 5 # Update number of outputs (nc + 5)\n",
        "\n",
        "\n",
        "# Start the training loop\n",
        "for epoch in range(1, total_epochs + 1):\n",
        "    print(f\"Starting epoch {epoch}/{total_epochs}\")\n",
        "\n",
        "    # Clean up previous sampled training data before sampling for the current epoch\n",
        "    shutil.rmtree(temp_train_images_dir)\n",
        "    shutil.rmtree(temp_train_labels_dir)\n",
        "    os.makedirs(temp_train_images_dir, exist_ok=True)\n",
        "    os.makedirs(temp_train_labels_dir, exist_ok=True)\n",
        "\n",
        "    # Randomly sample images for this epoch's training\n",
        "    sampled_images = random.sample(all_train_images, images_per_epoch)\n",
        "\n",
        "    # Copy the sampled images and their corresponding labels to the temporary training directory\n",
        "    for img_path in sampled_images:\n",
        "        img_name = os.path.basename(img_path)\n",
        "        label_name = img_name.replace('.jpeg', '.txt').replace('.jpg', '.txt').replace('.png', '.txt')\n",
        "        label_path = os.path.join('./extracted cmbined data/train/labels', label_name)\n",
        "\n",
        "        shutil.copy(img_path, temp_train_images_dir)\n",
        "        if os.path.exists(label_path):\n",
        "            shutil.copy(label_path, temp_train_labels_dir)\n",
        "\n",
        "    # Train the model for one epoch using the sampled data\n",
        "    # Load the last saved weights to resume training if they exist\n",
        "    last_weights_path = 'runs/detect/train/weights/last.pt' # Assuming the latest run is named 'train' by default\n",
        "    if os.path.exists(last_weights_path):\n",
        "        model = YOLO(last_weights_path)\n",
        "\n",
        "        # Access and modify the number of classes (nc) in the model based on the dataset\n",
        "        # The dataset has 43 classes, but some labels go up to 49. We set to 50 to be safe.\n",
        "        model.model.nc = data['nc'] if 'nc' in data and data['nc'] is not None else 43 # Default to 43 if nc is not specified or None\n",
        "        # Assuming the last layer is the detect layer, update its nc and no\n",
        "        if hasattr(model.model, 'model') and len(model.model.model) > 0 and hasattr(model.model.model[-1], 'nc'):\n",
        "            model.model.model[-1].nc = model.model.nc\n",
        "            model.model.model[-1].no = model.model.nc + 5 # Update number of outputs (nc + 5)\n",
        "\n",
        "    model.train(data=temp_data_yaml_path, epochs=1, imgsz=640, fliplr=0.5, degrees=10)\n",
        "\n",
        "\n",
        "    # Evaluate the model every eval_interval epochs\n",
        "    if epoch % eval_interval == 0:\n",
        "        print(f\"Evaluating model after epoch {epoch}...\")\n",
        "        metrics = model.val()\n",
        "\n",
        "        # Extract key metrics from the evaluation results\n",
        "        precision = metrics.results_dict['metrics/precision(B)']\n",
        "        recall = metrics.results_dict['metrics/recall(B)']\n",
        "        map50 = metrics.results_dict['metrics/mAP50(B)']\n",
        "        map50_95 = metrics.results_dict['metrics/mAP50-95(B)']\n",
        "\n",
        "        # Append the extracted metrics to the list\n",
        "        metrics_data.append({\n",
        "            'epoch': epoch,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'mAP@0.5': map50,\n",
        "            'mAP@0.5:0.95': map50_95\n",
        "        })\n",
        "\n",
        "        # Save the accumulated metrics to a CSV file\n",
        "        metrics_df = pd.DataFrame(metrics_data)\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"Metrics saved to {metrics_csv_path}\")\n",
        "\n",
        "\n",
        "print(\"Training and evaluation complete.\")\n",
        "\n",
        "# Clean up the main temporary data directory after training is finished\n",
        "shutil.rmtree(temp_data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}